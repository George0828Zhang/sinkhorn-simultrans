{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb434413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../fairseq\")\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"~/utility/imputer-pytorch\")\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54232d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq import (\n",
    "    checkpoint_utils,\n",
    "    options,\n",
    "    quantization_utils,\n",
    "    tasks,\n",
    "    utils,\n",
    ")\n",
    "from torchinfo import summary\n",
    "import logging\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch_imputer import best_alignment\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "plt.rc('mathtext', fontset=\"cm\")\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'].insert(0, 'PMINGLIU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6222f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_pmingliu():\n",
    "    module_path = Path(matplotlib.__file__).parent / \"mpl-data\" / \"fonts\" / \"ttf\" / \"PMINGLIU.ttf\"\n",
    "    url = \"https://www.wfonts.com/download/data/2015/07/28/pmingliu/pmingliu.zip\"\n",
    "    if module_path.exists():\n",
    "        print(\"PMINGLIU.ttf found.\")\n",
    "        return\n",
    "    if not Path(\"pmingliu.zip\").exists():\n",
    "        print(\"donwloading pmingliu.zip\")\n",
    "        !wget {url}\n",
    "    if not Path(\"pmingliu/PMINGLIU.ttf\").exists():\n",
    "        print(\"unzipping pmingliu.zip\")\n",
    "        !unzip -o pmingliu.zip\n",
    "    print(f\"copying PMINGLIU.ttf to {module_path}\")\n",
    "    !cp PMINGLIU.ttf {module_path}\n",
    "    !rm -rf ~/.cache/matplotlib\n",
    "    print(f\"cleaned matplotlib cache.\\nPLEASE RESTART JUPYTER SERVER.\")\n",
    "    \n",
    "fetch_pmingliu()\n",
    "plt.plot([1,2], label=\"測試\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=\"../expcwmt/checkpoints/sinkhorn_delay3_ft/checkpoint_best.pt\"\n",
    "inference_config_yaml=\"../expcwmt/infer_mt.yaml\"\n",
    "user_dir=\"/home/george/Projects/sinkhorn-simultrans/simultaneous_translation\"\n",
    "data_dir=\"/media/george/Data/cwmt/zh-en/data-bin\"\n",
    "use_cuda = True\n",
    "# max_tokens = 100\n",
    "batch_size = 20\n",
    "\n",
    "states = checkpoint_utils.load_checkpoint_to_cpu(\n",
    "    path=checkpoint, arg_overrides=None, load_on_all_ranks=False)\n",
    "cfg = states[\"cfg\"]\n",
    "cfg.task.inference_config_yaml = inference_config_yaml\n",
    "cfg.common.user_dir = user_dir\n",
    "cfg.task.data = data_dir\n",
    "# cfg.dataset.max_tokens_valid = max_tokens\n",
    "cfg.dataset.batch_size = batch_size\n",
    "cfg.model.load_pretrained_encoder_from = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logger = logging.getLogger(\"fairseq_cli.train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5cc1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.import_user_module(cfg.common)\n",
    "\n",
    "# Setup task, e.g., translation, language modeling, etc.\n",
    "task = tasks.setup_task(cfg.task)\n",
    "# Build model and criterion\n",
    "model = task.build_model(cfg.model)\n",
    "criterion = task.build_criterion(cfg.criterion)\n",
    "logger.info(summary(model))\n",
    "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
    "logger.info(\"model: {}\".format(model.__class__.__name__))\n",
    "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1641f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"loading model(s) from {}\".format(cfg.common_eval.path))\n",
    "model = task.build_model(cfg.model)\n",
    "model.load_state_dict(\n",
    "    states[\"model\"], strict=True, model_cfg=cfg.model\n",
    ")\n",
    "\n",
    "# Optimize ensemble for generation\n",
    "use_fp16 = False\n",
    "if use_cuda:\n",
    "    if torch.cuda.get_device_capability(0)[0] >= 7:\n",
    "        model.half()\n",
    "        use_fp16 = True\n",
    "    model.cuda()\n",
    "model.prepare_for_inference_(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.load_dataset(cfg.dataset.gen_subset, task_cfg=cfg.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "itr = task.get_batch_iterator(\n",
    "    dataset=task.dataset(cfg.dataset.gen_subset),\n",
    "    max_tokens=cfg.dataset.max_tokens,\n",
    "    max_sentences=cfg.dataset.batch_size,\n",
    "    max_positions=utils.resolve_max_positions(\n",
    "        task.max_positions(), model.max_positions() #*[m.max_positions() for m in models]\n",
    "    ),\n",
    "    ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n",
    "    required_batch_size_multiple=cfg.dataset.required_batch_size_multiple,\n",
    "    seed=cfg.common.seed,\n",
    "    num_shards=cfg.distributed_training.distributed_world_size,\n",
    "    shard_id=cfg.distributed_training.distributed_rank,\n",
    "    num_workers=cfg.dataset.num_workers,\n",
    "    data_buffer_size=cfg.dataset.data_buffer_size,\n",
    ").next_epoch_itr(shuffle=False)\n",
    "\n",
    "generator = task.build_generator(\n",
    "    [model], cfg.generation, extra_gen_cls_kwargs=None\n",
    ")\n",
    "\n",
    "# Handle tokenization and BPE\n",
    "def decode(\n",
    "    tensor,\n",
    "    dictionary,\n",
    "    escape_unk=False,\n",
    "    unk_string=None,\n",
    "    include_eos=False,\n",
    "    separator=None,\n",
    "):\n",
    "    def token_string(i):\n",
    "        if i == dictionary.unk():\n",
    "            if unk_string is not None:\n",
    "                return unk_string\n",
    "            else:\n",
    "                return dictionary.unk_string(escape_unk)\n",
    "        elif i == dictionary.bos():\n",
    "            return \"$\\phi$\"\n",
    "        else:\n",
    "            return dictionary[i]\n",
    "\n",
    "    sent = [token_string(i) for i in tensor]\n",
    "    return sent if separator is None else separator.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_half(t):\n",
    "    if t.dtype is torch.float32:\n",
    "        return t.half()\n",
    "    return t\n",
    "\n",
    "skips = 0\n",
    "for i,sample in enumerate(itr):\n",
    "    if use_cuda:\n",
    "        sample = utils.move_to_cuda(sample) \n",
    "        if use_fp16:\n",
    "            sample = utils.apply_to_sample(apply_half, sample)\n",
    "    if \"net_input\" not in sample or i<skips:\n",
    "        continue\n",
    "\n",
    "    break\n",
    "\n",
    "src_tokens = sample[\"net_input\"][\"src_tokens\"]\n",
    "src_lengths = sample[\"net_input\"][\"src_lengths\"]\n",
    "prev_output_tokens = sample[\"net_input\"][\"prev_output_tokens\"]\n",
    "target = sample[\"target\"]\n",
    "logits, extra = model.forward(src_tokens, src_lengths, prev_output_tokens=prev_output_tokens, return_all_hiddens=True)\n",
    "pred_argmax = logits.argmax(-1)\n",
    "[decode(t, task.src_dict) for t in src_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c414ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    net_output = (logits, extra)\n",
    "    lprobs = model.get_normalized_probs(\n",
    "        net_output, log_probs=True\n",
    "    )\n",
    "    bsz = target.size(0)\n",
    "    max_src = lprobs.size(1)\n",
    "    lprobs = lprobs.transpose(1, 0).contiguous()\n",
    "\n",
    "    # get subsampling padding mask & lengths\n",
    "    if net_output[1][\"padding_mask\"] is not None:\n",
    "        non_padding_mask = ~net_output[1][\"padding_mask\"]\n",
    "        input_lengths = non_padding_mask.long().sum(-1)\n",
    "    else:\n",
    "        input_lengths = lprobs.new_ones(\n",
    "            (bsz, max_src), dtype=torch.long).sum(-1)\n",
    "\n",
    "    pad_mask = (target != task.tgt_dict.pad()) & (\n",
    "       target != task.tgt_dict.eos()\n",
    "    )\n",
    "    #pad_mask = (target != task.tgt_dict.pad())\n",
    "    targets_flat = target.masked_select(pad_mask)\n",
    "    target_lengths = pad_mask.long().sum(-1)\n",
    "\n",
    "    forced_states = best_alignment(\n",
    "        lprobs, target, input_lengths, target_lengths, blank=0, zero_infinity=False\n",
    "    )\n",
    "    pred_forced = [\n",
    "        [0 if state % 2 == 0 else sent[state // 2] for state in states]\n",
    "        for states, sent in zip(forced_states, target)\n",
    "    ]\n",
    "    for x in pred_forced:\n",
    "        if len(x) < max_src:\n",
    "            x.extend([task.tgt_dict.pad()]*(max_src-len(x)))\n",
    "else:\n",
    "    pred_forced = pred_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable\n",
    "# from matplotlib import font_manager\n",
    "\n",
    "# fontP = font_manager.FontProperties()\n",
    "# fontP.set_family('SimHei')\n",
    "# fontP.set_size(14)\n",
    "\n",
    "def plot_align(\n",
    "    aln,\n",
    "    titles: Optional[list]=None,\n",
    "    x_tokens: Optional[list]=None,\n",
    "    y_tokens: Optional[list]=None,\n",
    "    norm_fn: Optional[Callable]=lambda x:x,\n",
    "    columns: Optional[int]=1,\n",
    "    tick_size: Optional[int]=10,\n",
    "    fig_size: Optional[tuple]=(12,8),\n",
    "    save: Optional[str]=None,\n",
    "    cmap=plt.cm.viridis #plt.cm.Blues,\n",
    "):\n",
    "    \"\"\"Function to plot the alignment with tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    n_graphs = len(aln)\n",
    "    rows = 1+(n_graphs // columns)\n",
    "    ylen, xlen = aln[0].shape\n",
    "    \n",
    "    if titles is None:\n",
    "        titles = [f\"example {i}\" for i in range(n_graphs)]\n",
    "    if x_tokens is None:\n",
    "        x_tokens = [range(xlen)]*n_graphs\n",
    "    if y_tokens is None:\n",
    "        y_tokens = [range(ylen)]*n_graphs    \n",
    "    \n",
    "    fig = plt.figure(figsize=(fig_size[0]*columns,fig_size[1]*rows), dpi=100) \n",
    "    for i,a in enumerate(aln):\n",
    "        ax = fig.add_subplot(rows, columns, i+1)\n",
    "        ax.imshow(norm_fn(a), cmap=cmap)\n",
    "        ax.set_title(titles[i%len(titles)]) # title\n",
    "        for tick in ax.get_xticklabels(): # diagonal xtick\n",
    "            tick.set_rotation(45)\n",
    "            #tick.set_fontproperties(fprop)        \n",
    "        #for tick in ax.get_yticklabels():\n",
    "            #tick.set_fontproperties(fprop)\n",
    "        ax.set_xticks(range(xlen)) \n",
    "        ax.set_xticklabels(x_tokens[i%len(x_tokens)], fontsize=tick_size)\n",
    "        ax.set_yticks(range(ylen)) \n",
    "        ax.set_yticklabels(y_tokens[i%len(y_tokens)], fontsize=tick_size)\n",
    "        \n",
    "     \n",
    "    if save is not None:\n",
    "        fig.savefig(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9e6c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "x_tokens = [decode(t, task.src_dict) for t in src_tokens[:N]]\n",
    "y_tokens = [decode(t, task.tgt_dict) for t in pred_forced[:N]]\n",
    "y_tokens = [\n",
    "    [l+\" \"+r for l, r in zip(sent[::2], sent[1::2])]\n",
    "    for sent in y_tokens]\n",
    "\n",
    "attn_weights = extra[\"attn\"][0].data.float().cpu()\n",
    "log_alpha = extra[\"log_alpha\"][0].data.float().cpu()\n",
    "b, n = attn_weights.shape[:2]\n",
    "plot_align(\n",
    "    attn_weights[:N],\n",
    "    x_tokens=x_tokens,\n",
    "    y_tokens=y_tokens,\n",
    "    titles = list(range(len(x_tokens))),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138fa48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from simultaneous_translation.modules.sinkhorn_attention import (\n",
    "    gumbel_sinkhorn\n",
    ")\n",
    "\n",
    "eid = 14\n",
    "\n",
    "log_alpha = extra[\"log_alpha\"][0].data.float().cpu()\n",
    "attn_weights = log_alpha\n",
    "new_key_padding_mask = src_tokens.eq(1).to(attn_weights.device)\n",
    "\n",
    "final_mask = new_key_padding_mask.unsqueeze(1) & (~new_key_padding_mask).unsqueeze(2)\n",
    "neg_inf = -torch.finfo(attn_weights.dtype).max\n",
    "# mask out normal -> pad attentions\n",
    "attn_weights = attn_weights.masked_fill(\n",
    "    final_mask,\n",
    "    neg_inf,\n",
    ")\n",
    "# mask out pad -> normal attentions\n",
    "attn_weights = attn_weights.masked_fill(\n",
    "    final_mask.transpose(2, 1),\n",
    "    neg_inf,\n",
    ")\n",
    "\n",
    "x_tokens = [decode(t, task.src_dict) for t in src_tokens[eid:eid+1]]\n",
    "y_tokens = [decode(t, task.tgt_dict) for t in pred_forced[eid:eid+1]]\n",
    "y_tokens = [\n",
    "    [l+\" \"+r for l, r in zip(sent[::2], sent[1::2])]\n",
    "    for sent in y_tokens]\n",
    "\n",
    "attn_weights = gumbel_sinkhorn(\n",
    "    attn_weights,\n",
    "    tau=0.25,\n",
    "    n_iter=16,\n",
    "    noise_factor=0.\n",
    ")\n",
    "\n",
    "remove_pad = True\n",
    "if remove_pad:\n",
    "    slen = src_lengths[eid]\n",
    "    plot_align(\n",
    "        attn_weights[eid][:slen,:slen].unsqueeze(0),\n",
    "        x_tokens=[x_tokens[0][:slen]],\n",
    "        y_tokens=[y_tokens[0][:slen]],\n",
    "        titles = [\"permutation matrix\"],\n",
    "        cmap=plt.cm.Blues,\n",
    "        tick_size=18\n",
    "    )\n",
    "else:\n",
    "    plot_align(\n",
    "        attn_weights[eid].unsqueeze(0),\n",
    "        x_tokens=x_tokens,\n",
    "        y_tokens=y_tokens,\n",
    "        titles = [\"permutation matrix\"],\n",
    "        cmap=plt.cm.Blues,\n",
    "        tick_size=18\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e2d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_logits, _ = model.forward_causal(src_tokens, src_lengths)\n",
    "causal_pred_argmax = causal_logits.argmax(-1)\n",
    "print(decode(causal_pred_argmax[eid:eid+1][0], task.tgt_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb10aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2c5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fair]",
   "language": "python",
   "name": "conda-env-fair-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
