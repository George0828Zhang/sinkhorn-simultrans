{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../fairseq\")\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"~/utility/imputer-pytorch\")\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/fairseq/blob/master/fairseq_cli/generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq import (\n",
    "    checkpoint_utils,\n",
    "    options,\n",
    "    quantization_utils,\n",
    "    tasks,\n",
    "    utils,\n",
    ")\n",
    "from torchinfo import summary\n",
    "import logging\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch_imputer import best_alignment\n",
    "from simultaneous_translation.modules.sinkhorn_attention import (\n",
    "    gumbel_sinkhorn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=\"../exp/checkpoints/toy_mt_proj/checkpoint_best.pt\"\n",
    "inference_config_yaml=\"../exp/infer_mt.yaml\"\n",
    "use_cuda = False\n",
    "\n",
    "states = checkpoint_utils.load_checkpoint_to_cpu(\n",
    "    path=checkpoint, arg_overrides=None, load_on_all_ranks=False)\n",
    "cfg = states[\"cfg\"]\n",
    "cfg.task.inference_config_yaml = inference_config_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logger = logging.getLogger(\"fairseq_cli.train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.import_user_module(cfg.common)\n",
    "\n",
    "# Setup task, e.g., translation, language modeling, etc.\n",
    "task = tasks.setup_task(cfg.task)\n",
    "# Build model and criterion\n",
    "model = task.build_model(cfg.model)\n",
    "criterion = task.build_criterion(cfg.criterion)\n",
    "logger.info(summary(model))\n",
    "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
    "logger.info(\"model: {}\".format(model.__class__.__name__))\n",
    "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"loading model(s) from {}\".format(cfg.common_eval.path))\n",
    "model = task.build_model(cfg.model)\n",
    "model.load_state_dict(\n",
    "    states[\"model\"], strict=True, model_cfg=cfg.model\n",
    ")\n",
    "\n",
    "# Optimize ensemble for generation\n",
    "use_fp16 = False\n",
    "if use_cuda:\n",
    "    if torch.cuda.get_device_capability(0)[0] >= 7:\n",
    "        model.half()\n",
    "        use_fp16 = True\n",
    "    model.cuda()\n",
    "model.prepare_for_inference_(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.load_dataset(cfg.dataset.gen_subset, task_cfg=cfg.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "itr = task.get_batch_iterator(\n",
    "    dataset=task.dataset(cfg.dataset.gen_subset),\n",
    "    max_tokens=cfg.dataset.max_tokens,\n",
    "    max_sentences=cfg.dataset.batch_size,\n",
    "    max_positions=utils.resolve_max_positions(\n",
    "        task.max_positions(), model.max_positions() #*[m.max_positions() for m in models]\n",
    "    ),\n",
    "    ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n",
    "    required_batch_size_multiple=cfg.dataset.required_batch_size_multiple,\n",
    "    seed=cfg.common.seed,\n",
    "    num_shards=cfg.distributed_training.distributed_world_size,\n",
    "    shard_id=cfg.distributed_training.distributed_rank,\n",
    "    num_workers=cfg.dataset.num_workers,\n",
    "    data_buffer_size=cfg.dataset.data_buffer_size,\n",
    ").next_epoch_itr(shuffle=False)\n",
    "\n",
    "generator = task.build_generator(\n",
    "    [model], cfg.generation, extra_gen_cls_kwargs=None\n",
    ")\n",
    "\n",
    "# Handle tokenization and BPE\n",
    "def decode(\n",
    "    tensor,\n",
    "    escape_unk=False,\n",
    "    unk_string=None,\n",
    "    include_eos=False,\n",
    "    separator=None,\n",
    "):    \n",
    "    def token_string(i):\n",
    "        if i == task.tgt_dict.unk():\n",
    "            if unk_string is not None:\n",
    "                return unk_string\n",
    "            else:\n",
    "                return task.tgt_dict.unk_string(escape_unk)\n",
    "        else:\n",
    "            return task.tgt_dict[i]\n",
    "\n",
    "    sent = [token_string(i) for i in tensor]\n",
    "    return sent if separator is None else separator.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_half(t):\n",
    "    if t.dtype is torch.float32:\n",
    "        return t.half()\n",
    "    return t\n",
    "\n",
    "skips = 10\n",
    "for i,sample in enumerate(itr):\n",
    "    if use_cuda:\n",
    "        sample = utils.move_to_cuda(sample) \n",
    "        if use_fp16:\n",
    "            sample = utils.apply_to_sample(apply_half, sample)\n",
    "    if \"net_input\" not in sample or i<skips:\n",
    "        continue\n",
    "\n",
    "    break\n",
    "\n",
    "src_tokens = sample[\"net_input\"][\"src_tokens\"]\n",
    "src_lengths = sample[\"net_input\"][\"src_lengths\"]\n",
    "target = sample[\"target\"]\n",
    "logits, extra = model.forward(src_tokens, src_lengths, return_all_hiddens=True)\n",
    "pred_argmax = logits.argmax(-1)\n",
    "[decode(t) for t in src_tokens][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    net_output = (logits, extra)\n",
    "    lprobs = model.get_normalized_probs(\n",
    "        net_output, log_probs=True\n",
    "    )\n",
    "    bsz = target.size(0)\n",
    "    max_src = lprobs.size(1)\n",
    "    lprobs = lprobs.transpose(1, 0).contiguous()\n",
    "\n",
    "    # get subsampling padding mask & lengths\n",
    "    if net_output[1][\"padding_mask\"] is not None:\n",
    "        non_padding_mask = ~net_output[1][\"padding_mask\"]\n",
    "        input_lengths = non_padding_mask.long().sum(-1)\n",
    "    else:\n",
    "        input_lengths = lprobs.new_ones(\n",
    "            (bsz, max_src), dtype=torch.long).sum(-1)\n",
    "\n",
    "    pad_mask = (target != task.tgt_dict.pad()) & (\n",
    "        target != task.tgt_dict.eos()\n",
    "    )\n",
    "    targets_flat = target.masked_select(pad_mask)\n",
    "    target_lengths = pad_mask.long().sum(-1)\n",
    "\n",
    "    forced_states = best_alignment(\n",
    "        lprobs, target, input_lengths, target_lengths, blank=0, zero_infinity=False\n",
    "    )\n",
    "    pred_forced = [\n",
    "        [0 if state % 2 == 0 else sent[state // 2] for state in states]\n",
    "        for states, sent in zip(forced_states, target)\n",
    "    ]\n",
    "else:\n",
    "    pred_forced = pred_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable\n",
    "\n",
    "def plot_align(\n",
    "    aln,\n",
    "    titles: Optional[list]=None,\n",
    "    x_tokens: Optional[list]=None,\n",
    "    y_tokens: Optional[list]=None,\n",
    "    norm_fn: Optional[Callable]=lambda x:x,\n",
    "    columns: Optional[int]=1,\n",
    "    tick_size: Optional[int]=10,\n",
    "    fig_size: Optional[tuple]=(12,8),\n",
    "    save: Optional[str]=None,\n",
    "    cmap=plt.cm.viridis #plt.cm.Blues,\n",
    "):\n",
    "    \"\"\"Function to plot the alignment with tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    n_graphs = len(aln)\n",
    "    rows = 1+(n_graphs // columns)\n",
    "    ylen, xlen = aln[0].shape\n",
    "    \n",
    "    if titles is None:\n",
    "        titles = [f\"example {i}\" for i in range(n_graphs)]\n",
    "    if x_tokens is None:\n",
    "        x_tokens = [range(xlen)]*n_graphs\n",
    "    if y_tokens is None:\n",
    "        y_tokens = [range(ylen)]*n_graphs    \n",
    "    \n",
    "    fig = plt.figure(figsize=(fig_size[0]*columns,fig_size[1]*rows), dpi=100) \n",
    "    for i,a in enumerate(aln):\n",
    "        ax = fig.add_subplot(rows, columns, i+1)\n",
    "        ax.imshow(norm_fn(a), cmap=cmap)\n",
    "        ax.set_title(titles[i%len(titles)]) # title\n",
    "        for tick in ax.get_xticklabels(): # diagonal xtick\n",
    "            tick.set_rotation(45)\n",
    "        ax.set_xticks(range(xlen)) \n",
    "        ax.set_xticklabels(x_tokens[i%len(x_tokens)], fontsize=tick_size)\n",
    "        ax.set_yticks(range(ylen)) \n",
    "        ax.set_yticklabels(y_tokens[i%len(y_tokens)], fontsize=tick_size)\n",
    "     \n",
    "    if save is not None:\n",
    "        fig.savefig(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "x_tokens = [decode(t) for t in src_tokens[:N]]\n",
    "y_tokens = [decode(t) for t in pred_forced[:N]]\n",
    "y_tokens = [\n",
    "    [l+r for l, r in zip(sent[::2], sent[1::2])]\n",
    "    for sent in y_tokens]\n",
    "\n",
    "attn_weights = extra[\"attn\"][0].data.float().cpu()\n",
    "log_alpha = extra[\"log_alpha\"][0].data.float().cpu()\n",
    "b, n = attn_weights.shape[:2]\n",
    "plot_align(\n",
    "    attn_weights[:N],\n",
    "#     fig_size=(13,13),\n",
    "    x_tokens=x_tokens,\n",
    "    y_tokens=y_tokens,\n",
    "    titles = [\"transport map\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "attn_weights = gumbel_sinkhorn(\n",
    "    log_alpha,\n",
    "    tau=0.02,\n",
    "    n_iter=8,\n",
    "    noise_factor=0.\n",
    ")\n",
    "plot_align(\n",
    "    attn_weights[:N],\n",
    "#     fig_size=(13,13),\n",
    "    x_tokens=x_tokens,\n",
    "    y_tokens=y_tokens,\n",
    "    titles = [\"transport map\", \"cost\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_sinkhorn(log_alpha: torch.Tensor, tau: float = 0.7, n_iter: int = 20, noise: bool = True):\n",
    "    \"\"\"\n",
    "    https://github.com/gpeyre/SinkhornAutoDiff/blob/master/sinkhorn_pointcloud.py\n",
    "    \"\"\"\n",
    "    b, _, n = log_alpha.size()\n",
    "    mu = log_alpha.new_ones((b, n)) / n\n",
    "    nu = log_alpha.new_ones((b, n)) / n\n",
    "    epsilon = tau\n",
    "    C = -log_alpha\n",
    "    \n",
    "    # Parameters of the Sinkhorn algorithm.\n",
    "    rho = 1  # (.5) **2          # unbalanced transport\n",
    "    tau = -.8  # nesterov-like acceleration\n",
    "    lam = rho / (rho + epsilon)  # Update exponent\n",
    "    thresh = 10**(-1)  # stopping criterion\n",
    "\n",
    "    # Elementary operations .....................................................................\n",
    "    def ave(u, u1):\n",
    "        \"Barycenter subroutine, used by kinetic acceleration through extrapolation.\"\n",
    "        return tau * u + (1 - tau) * u1\n",
    "\n",
    "    def M(u, v):\n",
    "        \"Modified cost for logarithmic updates\"\n",
    "        \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\n",
    "        return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / epsilon\n",
    "\n",
    "    def lse(A, dim):\n",
    "        \"log-sum-exp\"\n",
    "        return torch.logsumexp(A, dim, keepdim=True)\n",
    "        #return torch.log(torch.exp(A).sum(1, keepdim=True) + 1e-6)  # add 10^-6 to prevent NaN\n",
    "\n",
    "    # Actual Sinkhorn loop ......................................................................\n",
    "    u, v, err = 0. * mu, 0. * nu, 0.\n",
    "    actual_nits = 0  # to check if algorithm terminates because of threshold or max iterations reached\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        u1 = u  # useful to check the update\n",
    "        u = epsilon * (torch.log(mu) - lse(M(u, v), -1).squeeze()) + u\n",
    "        v = epsilon * (torch.log(nu) - lse(M(u, v), -2).squeeze()) + v\n",
    "        # accelerated unbalanced iterations\n",
    "        # u = ave( u, lam * ( epsilon * ( torch.log(mu) - lse(M(u,v)).squeeze()   ) + u ) )\n",
    "        # v = ave( v, lam * ( epsilon * ( torch.log(nu) - lse(M(u,v).t()).squeeze() ) + v ) )\n",
    "        err = (u - u1).abs().sum()\n",
    "\n",
    "        actual_nits += 1\n",
    "        if (err < thresh).data.numpy():\n",
    "            break\n",
    "    U, V = u, v\n",
    "    pi = torch.exp(M(U, V))  # Transport plan pi = diag(a)*K*diag(b)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:apex]",
   "language": "python",
   "name": "conda-env-apex-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
