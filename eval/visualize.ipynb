{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../fairseq\")\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"~/utility/imputer-pytorch\")\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/fairseq/blob/master/fairseq_cli/generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq import (\n",
    "    checkpoint_utils,\n",
    "    options,\n",
    "    quantization_utils,\n",
    "    tasks,\n",
    "    utils,\n",
    ")\n",
    "from torchinfo import summary\n",
    "import logging\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch_imputer import best_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=\"../exp/checkpoints/toy_attn_sort_ctc/checkpoint_best.pt\"\n",
    "inference_config_yaml=\"../exp/infer_toy.yaml\"\n",
    "use_cuda = True\n",
    "\n",
    "states = checkpoint_utils.load_checkpoint_to_cpu(\n",
    "    path=checkpoint, arg_overrides=None, load_on_all_ranks=False)\n",
    "cfg = states[\"cfg\"]\n",
    "cfg.task.inference_config_yaml = inference_config_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logger = logging.getLogger(\"fairseq_cli.train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.import_user_module(cfg.common)\n",
    "\n",
    "# Setup task, e.g., translation, language modeling, etc.\n",
    "task = tasks.setup_task(cfg.task)\n",
    "# Build model and criterion\n",
    "model = task.build_model(cfg.model)\n",
    "criterion = task.build_criterion(cfg.criterion)\n",
    "logger.info(summary(model))\n",
    "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
    "logger.info(\"model: {}\".format(model.__class__.__name__))\n",
    "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"loading model(s) from {}\".format(cfg.common_eval.path))\n",
    "model = task.build_model(cfg.model)\n",
    "model.load_state_dict(\n",
    "    states[\"model\"], strict=True, model_cfg=cfg.model\n",
    ")\n",
    "\n",
    "# Optimize ensemble for generation\n",
    "if use_cuda:\n",
    "    model.half()\n",
    "    model.cuda()\n",
    "model.prepare_for_inference_(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.load_dataset(cfg.dataset.gen_subset, task_cfg=cfg.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "itr = task.get_batch_iterator(\n",
    "    dataset=task.dataset(cfg.dataset.gen_subset),\n",
    "    max_tokens=cfg.dataset.max_tokens,\n",
    "    max_sentences=cfg.dataset.batch_size,\n",
    "    max_positions=utils.resolve_max_positions(\n",
    "        task.max_positions(), model.max_positions() #*[m.max_positions() for m in models]\n",
    "    ),\n",
    "    ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n",
    "    required_batch_size_multiple=cfg.dataset.required_batch_size_multiple,\n",
    "    seed=cfg.common.seed,\n",
    "    num_shards=cfg.distributed_training.distributed_world_size,\n",
    "    shard_id=cfg.distributed_training.distributed_rank,\n",
    "    num_workers=cfg.dataset.num_workers,\n",
    "    data_buffer_size=cfg.dataset.data_buffer_size,\n",
    ").next_epoch_itr(shuffle=False)\n",
    "\n",
    "generator = task.build_generator(\n",
    "    [model], cfg.generation, extra_gen_cls_kwargs=None\n",
    ")\n",
    "\n",
    "# Handle tokenization and BPE\n",
    "def decode(\n",
    "    tensor,\n",
    "    escape_unk=False,\n",
    "    unk_string=None,\n",
    "    include_eos=False,\n",
    "    separator=None,\n",
    "):    \n",
    "    def token_string(i):\n",
    "        if i == task.tgt_dict.unk():\n",
    "            if unk_string is not None:\n",
    "                return unk_string\n",
    "            else:\n",
    "                return task.tgt_dict.unk_string(escape_unk)\n",
    "        else:\n",
    "            return task.tgt_dict[i]\n",
    "\n",
    "    sent = [token_string(i) for i in tensor]\n",
    "    return sent if separator is None else separator.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in itr:\n",
    "    if use_cuda:\n",
    "        sample = utils.move_to_cuda(sample) \n",
    "    if \"net_input\" not in sample:\n",
    "        continue\n",
    "\n",
    "    break\n",
    "\n",
    "src_tokens = sample[\"net_input\"][\"src_tokens\"]\n",
    "src_lengths = sample[\"net_input\"][\"src_lengths\"]\n",
    "target = sample[\"target\"]\n",
    "logits, extra = model.forward(src_tokens, src_lengths, return_all_hiddens=True)\n",
    "pred_argmax = logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_output = (logits, extra)\n",
    "lprobs = model.get_normalized_probs(\n",
    "    net_output, log_probs=True\n",
    ")\n",
    "bsz = target.size(0)\n",
    "max_src = lprobs.size(1)\n",
    "lprobs = lprobs.transpose(1, 0).contiguous()\n",
    "\n",
    "# get subsampling padding mask & lengths\n",
    "if net_output[1][\"padding_mask\"] is not None:\n",
    "    non_padding_mask = ~net_output[1][\"padding_mask\"]\n",
    "    input_lengths = non_padding_mask.long().sum(-1)\n",
    "else:\n",
    "    input_lengths = lprobs.new_ones(\n",
    "        (bsz, max_src), dtype=torch.long).sum(-1)\n",
    "\n",
    "pad_mask = (target != task.tgt_dict.pad()) & (\n",
    "    target != task.tgt_dict.eos()\n",
    ")\n",
    "targets_flat = target.masked_select(pad_mask)\n",
    "target_lengths = pad_mask.long().sum(-1)\n",
    "\n",
    "forced_states = best_alignment(\n",
    "    lprobs, target, input_lengths, target_lengths, blank=0, zero_infinity=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_argmax = [\n",
    "    [0 if state % 2 == 0 else sent[state // 2] for state in states]\n",
    "    for states, sent in zip(forced_states, target)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable\n",
    "\n",
    "def plot_align(\n",
    "    aln,\n",
    "    titles: Optional[list]=None,\n",
    "    x_tokens: Optional[list]=None,\n",
    "    y_tokens: Optional[list]=None,\n",
    "    norm_fn: Optional[Callable]=lambda x:x,\n",
    "    columns: Optional[int]=1,\n",
    "    tick_size: Optional[int]=10,\n",
    "    fig_size: Optional[tuple]=(12,8),\n",
    "    save: Optional[str]=None,\n",
    "    cmap=plt.cm.viridis #plt.cm.Blues,\n",
    "):\n",
    "    \"\"\"Function to plot the alignment with tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    n_graphs = len(aln)\n",
    "    rows = 1+(n_graphs // columns)\n",
    "    ylen, xlen = aln[0].shape\n",
    "    \n",
    "    if titles is None:\n",
    "        titles = [f\"example {i}\" for i in range(n_graphs)]\n",
    "    if x_tokens is None:\n",
    "        x_tokens = [range(xlen)]*n_graphs\n",
    "    if y_tokens is None:\n",
    "        y_tokens = [range(ylen)]*n_graphs    \n",
    "    \n",
    "    fig = plt.figure(figsize=(fig_size[0]*columns,fig_size[1]*rows), dpi=100) \n",
    "    for i,a in enumerate(aln):\n",
    "        ax = fig.add_subplot(rows, columns, i+1)\n",
    "        ax.imshow(norm_fn(a), cmap=cmap)\n",
    "        ax.set_title(titles[i]) # title\n",
    "        for tick in ax.get_xticklabels(): # diagonal xtick\n",
    "            tick.set_rotation(45)\n",
    "        ax.set_xticks(range(xlen)) \n",
    "        ax.set_xticklabels(x_tokens[i%len(x_tokens)], fontsize=tick_size)\n",
    "        ax.set_yticks(range(ylen)) \n",
    "        ax.set_yticklabels(y_tokens[i%len(y_tokens)], fontsize=tick_size)\n",
    "     \n",
    "    if save is not None:\n",
    "        fig.savefig(save)\n",
    "        \n",
    "def get_alphabet():\n",
    "    \"\"\" credit goes to \n",
    "    https://stackoverflow.com/questions/1477294/generate-random-utf-8-string-in-python \"\"\"\n",
    "\n",
    "    try:\n",
    "        get_char = unichr\n",
    "    except NameError:\n",
    "        get_char = chr\n",
    "\n",
    "    include_ranges = [\n",
    "        (0x0021, 0x0021),\n",
    "        (0x0023, 0x0026),\n",
    "        (0x0028, 0x007E),\n",
    "        (0x00A1, 0x00AC),\n",
    "        (0x00AE, 0x00FF),\n",
    "        (0x0100, 0x017F),\n",
    "        (0x0180, 0x024F),\n",
    "        (0x2C60, 0x2C7F),\n",
    "        (0x16A0, 0x16F0),\n",
    "        (0x0370, 0x0377),\n",
    "        (0x037A, 0x037E),\n",
    "        (0x0384, 0x038A),\n",
    "        (0x038C, 0x038C),\n",
    "    ]\n",
    "\n",
    "    alphabet = [\n",
    "        get_char(code_point) for current_range in include_ranges\n",
    "        for code_point in range(current_range[0], current_range[1] + 1)\n",
    "    ]\n",
    "    return alphabet\n",
    "\n",
    "def chrs2ids(chrs, otype=str):\n",
    "    alphabet = get_alphabet()\n",
    "    return [otype(alphabet.index(c)) if c in alphabet else otype(c) for c in chrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 1\n",
    "\n",
    "x_tokens = [chrs2ids(decode(t)) for t in src_tokens[:N]]\n",
    "# y_tokens = [decode(t) for t in target[:N]]\n",
    "y_tokens = [decode(t) for t in pred_argmax[:N]]\n",
    "\n",
    "attn_weights = extra[\"attn\"][0].data.float().cpu()\n",
    "log_alpha = extra[\"log_alpha\"][0].data.float().cpu()\n",
    "b, n = attn_weights.shape[:2]\n",
    "plot_align(\n",
    "    torch.stack((attn_weights, -log_alpha), dim=1).view(2*b, n, n)[:2*N],\n",
    "    fig_size=(13,13),\n",
    "    x_tokens=x_tokens,\n",
    "    y_tokens=y_tokens,\n",
    "    titles = [\"transport map\", \"cost\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simultaneous_translation.modules.sinkhorn_attention import (\n",
    "    gumbel_sinkhorn\n",
    ")\n",
    "\n",
    "# encoder_states = extra[\"encoder_out\"][\"encoder_states\"]\n",
    "# non_causal_states = encoder_states[-2].transpose(1,0)\n",
    "# causal_states = encoder_states[-4].transpose(1,0)\n",
    "\n",
    "# causal outputs\n",
    "causal_out = model.encoder.forward_causal(src_tokens, src_lengths)\n",
    "causal_states = x = causal_out[\"encoder_out\"][0]\n",
    "encoder_padding_mask = causal_out[\"encoder_padding_mask\"][0] \\\n",
    "    if len(causal_out[\"encoder_padding_mask\"]) > 0 else None\n",
    "encoder_states = causal_out[\"encoder_states\"]\n",
    "\n",
    "# forward non-causal layers\n",
    "for layer in model.encoder.non_causal_layers:\n",
    "    x = layer(x, encoder_padding_mask)\n",
    "non_causal_states = x\n",
    "\n",
    "# reorder using sinkhorn layers\n",
    "_, attn, log_alpha = model.encoder.sinkhorn_layer(\n",
    "    query=x,  # this is non_causal_states\n",
    "    key=causal_states,\n",
    "    value=causal_states,\n",
    "    key_padding_mask=encoder_padding_mask,\n",
    ")\n",
    "\n",
    "# q = x.transpose(0, 1)\n",
    "# k = causal_states.transpose(0, 1)\n",
    "\n",
    "# energy_fn = \"l2\"\n",
    "# if energy_fn == \"dot\":\n",
    "#     attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "# elif energy_fn == \"cos\":\n",
    "#     # serious underflow for half.\n",
    "#     attn_weights = torch.cosine_similarity(\n",
    "#         q.float().unsqueeze(2),  # (bsz, tgt_len, 1, embed_dim)\n",
    "#         k.float().unsqueeze(1),  # (bsz, 1, src_len, embed_dim)\n",
    "#         dim=-1,\n",
    "#     )\n",
    "# elif energy_fn == \"l2\":\n",
    "#     # cdist not inplemented for half.\n",
    "#     attn_weights = -torch.cdist(q.float(), k.float(), p=2)\n",
    "# attn_weights = attn_weights.type_as(q)\n",
    "\n",
    "# log_alpha = attn_weights.clone()\n",
    "# attn = gumbel_sinkhorn(\n",
    "#     attn_weights.float(),\n",
    "#     tau=0.75,\n",
    "#     n_iter=8,\n",
    "#     noise_factor=0\n",
    "# )\n",
    "\n",
    "attn_weights = attn.data.float().cpu()\n",
    "log_alpha = log_alpha.data.float().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "attn_weights = gumbel_sinkhorn(\n",
    "    log_alpha,\n",
    "    tau=0.1,\n",
    "    n_iter=8,\n",
    "    noise_factor=0.\n",
    ")\n",
    "b, n = attn_weights.shape[:2]\n",
    "plot_align(\n",
    "    torch.stack((attn_weights, -log_alpha), dim=1).view(2*b, n, n)[:2*N],\n",
    "    fig_size=(13,13),\n",
    "    x_tokens=x_tokens,\n",
    "    y_tokens=y_tokens,\n",
    "    titles = [\"transport map\", \"cost\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_sinkhorn(log_alpha: torch.Tensor, tau: float = 0.7, n_iter: int = 20, noise: bool = True):\n",
    "    \"\"\"\n",
    "    https://github.com/gpeyre/SinkhornAutoDiff/blob/master/sinkhorn_pointcloud.py\n",
    "    \"\"\"\n",
    "    b, _, n = log_alpha.size()\n",
    "    mu = log_alpha.new_ones((b, n)) / n\n",
    "    nu = log_alpha.new_ones((b, n)) / n\n",
    "    epsilon = tau\n",
    "    C = -log_alpha\n",
    "    \n",
    "    # Parameters of the Sinkhorn algorithm.\n",
    "    rho = 1  # (.5) **2          # unbalanced transport\n",
    "    tau = -.8  # nesterov-like acceleration\n",
    "    lam = rho / (rho + epsilon)  # Update exponent\n",
    "    thresh = 10**(-1)  # stopping criterion\n",
    "\n",
    "    # Elementary operations .....................................................................\n",
    "    def ave(u, u1):\n",
    "        \"Barycenter subroutine, used by kinetic acceleration through extrapolation.\"\n",
    "        return tau * u + (1 - tau) * u1\n",
    "\n",
    "    def M(u, v):\n",
    "        \"Modified cost for logarithmic updates\"\n",
    "        \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\n",
    "        return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / epsilon\n",
    "\n",
    "    def lse(A, dim):\n",
    "        \"log-sum-exp\"\n",
    "        return torch.logsumexp(A, dim, keepdim=True)\n",
    "        #return torch.log(torch.exp(A).sum(1, keepdim=True) + 1e-6)  # add 10^-6 to prevent NaN\n",
    "\n",
    "    # Actual Sinkhorn loop ......................................................................\n",
    "    u, v, err = 0. * mu, 0. * nu, 0.\n",
    "    actual_nits = 0  # to check if algorithm terminates because of threshold or max iterations reached\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        u1 = u  # useful to check the update\n",
    "        u = epsilon * (torch.log(mu) - lse(M(u, v), -1).squeeze()) + u\n",
    "        v = epsilon * (torch.log(nu) - lse(M(u, v), -2).squeeze()) + v\n",
    "        # accelerated unbalanced iterations\n",
    "        # u = ave( u, lam * ( epsilon * ( torch.log(mu) - lse(M(u,v)).squeeze()   ) + u ) )\n",
    "        # v = ave( v, lam * ( epsilon * ( torch.log(nu) - lse(M(u,v).t()).squeeze() ) + v ) )\n",
    "        err = (u - u1).abs().sum()\n",
    "\n",
    "        actual_nits += 1\n",
    "        if (err < thresh).data.numpy():\n",
    "            break\n",
    "    U, V = u, v\n",
    "    pi = torch.exp(M(U, V))  # Transport plan pi = diag(a)*K*diag(b)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:apex]",
   "language": "python",
   "name": "conda-env-apex-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
